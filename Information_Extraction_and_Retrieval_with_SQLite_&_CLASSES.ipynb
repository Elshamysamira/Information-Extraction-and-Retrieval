{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elshamysamira/Information-Extraction-and-Retrieval/blob/sami/Information_Extraction_and_Retrieval_with_SQLite_%26_CLASSES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Version with SQLite and Classes**"
      ],
      "metadata": {
        "id": "-kZ2kjE48rih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install chardet"
      ],
      "metadata": {
        "id": "nAD9_lAC4TM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import chardet\n",
        "nltk.download('punkt')  # This downloads necessary datasets for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sqlite3\n",
        "import re"
      ],
      "metadata": {
        "id": "u42BpW7u6e8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "KEFhNtX24btr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "IXDGnNeo6lgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndex:\n",
        "    def __init__(self, db_path):\n",
        "        self.db_path = db_path\n",
        "        self.conn = sqlite3.connect(self.db_path)\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.create_table()\n",
        "\n",
        "    def create_table(self):\n",
        "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS inverted_index (\n",
        "                                word TEXT PRIMARY KEY,\n",
        "                                document_ids TEXT\n",
        "                            )''')\n",
        "\n",
        "    def save_index(self, inverted_index):\n",
        "        for word, document_ids in inverted_index.items():\n",
        "            doc_ids_str = ','.join(str(doc_id) for doc_id in document_ids)\n",
        "            self.cursor.execute(\"INSERT INTO inverted_index (word, document_ids) VALUES (?, ?)\", (word, doc_ids_str))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def close_connection(self):\n",
        "        self.conn.close()"
      ],
      "metadata": {
        "id": "WosMhu9e4fqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentTokenizer:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "\n",
        "    def tokenize(self):\n",
        "        tokenized_docs = {}\n",
        "        for documentID, document_path in enumerate(self.documents):\n",
        "            try:\n",
        "                with open(document_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    document_content = file.read()\n",
        "\n",
        "                tokens = word_tokenize(document_content)\n",
        "                tokenized_docs[documentID] = tokens\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "        return tokenized_docs\n"
      ],
      "metadata": {
        "id": "f2iCHrAD4lKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndexBuilder:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.inverted_index = defaultdict(set)\n",
        "\n",
        "    def build_index(self):\n",
        "        for documentID, document_path in enumerate(self.documents):\n",
        "            try:\n",
        "                with open(document_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    document_content = file.read()\n",
        "\n",
        "                for word in document_content.lower().split():\n",
        "                    self.inverted_index[word].add(documentID)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "    def get_index(self):\n",
        "        return self.inverted_index"
      ],
      "metadata": {
        "id": "JrO4ALSw4our"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchEngine:\n",
        "    def __init__(self, index_db_path, documents_mapping):\n",
        "        self.index_db_path = index_db_path\n",
        "        self.documents_mapping = documents_mapping\n",
        "\n",
        "    def lookup_word(self, word):\n",
        "        conn = sqlite3.connect(self.index_db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT document_ids FROM inverted_index WHERE word=?\", (word,))\n",
        "        result = cursor.fetchone()\n",
        "        conn.close()\n",
        "\n",
        "        if result:\n",
        "            document_ids = set(map(int, result[0].split(',')))\n",
        "            return document_ids\n",
        "        else:\n",
        "            return set()\n",
        "\n",
        "    def preprocess_query(self, query):\n",
        "        # Replace common contractions with their full forms\n",
        "        query = re.sub(r\"can't\", \"cannot\", query, flags=re.IGNORECASE)\n",
        "        query = re.sub(r\"n't\", \" not\", query, flags=re.IGNORECASE)  # Replace \"n't\" with \" not\" (e.g., \"can't\" -> \"cannot\")\n",
        "        # Add more replacements as needed for other contractions\n",
        "\n",
        "        return query\n",
        "\n",
        "    def search(self, query):\n",
        "        # Preprocess the query\n",
        "        query = self.preprocess_query(query)\n",
        "\n",
        "        # Tokenize the query\n",
        "        query_tokens = word_tokenize(query.lower())\n",
        "        print(f\"Tokenized Query: {query_tokens}\")  # Print tokenized query\n",
        "\n",
        "        # Look up each token in the inverted index\n",
        "        document_sets = [self.lookup_word(token) for token in query_tokens]\n",
        "        common_documents = set.intersection(*document_sets) if document_sets else set()\n",
        "\n",
        "        if common_documents:\n",
        "            print(f\"Congratulations! The word(s) '{query}' appear together in the following document ID(s): {common_documents}\")\n",
        "            for doc_id in common_documents:\n",
        "                print(f\"Document ID: {doc_id}, Document Name: {self.documents_mapping.get(doc_id, 'Unknown')}\")\n",
        "        else:\n",
        "            print(f\"I'm sorry, the word(s) '{query}' do not appear together in any document.\")\n"
      ],
      "metadata": {
        "id": "I4Eymjv04tuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentManager:\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "    def get_document_paths(self):\n",
        "        return [os.path.join(self.folder_path, file) for file in os.listdir(self.folder_path)]"
      ],
      "metadata": {
        "id": "9-VOdHWg4wab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/Documents'\n",
        "index_db_path = '/content/drive/My Drive/Documents/inverted_index.db'\n",
        "\n",
        "# Initialize DocumentManager\n",
        "doc_manager = DocumentManager(folder_path)\n",
        "document_paths = doc_manager.get_document_paths()"
      ],
      "metadata": {
        "id": "ujnxAyfn78TY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize documents\n",
        "doc_tokenizer = DocumentTokenizer(document_paths)\n",
        "tokenized_docs = doc_tokenizer.tokenize()"
      ],
      "metadata": {
        "id": "0AO-fxlf8AYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenized documents\n",
        "output_directory = '/content/drive/My Drive/Documents/tokenized'\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "for doc_id, tokens in tokenized_docs.items():\n",
        "    output_file_path = os.path.join(output_directory, f\"tokenized_document_{doc_id}.txt\")\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(' '.join(tokens))"
      ],
      "metadata": {
        "id": "9usR7FEQ8EFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build inverted index\n",
        "index_builder = InvertedIndexBuilder(document_paths)\n",
        "index_builder.build_index()\n",
        "inverted_index = index_builder.get_index()"
      ],
      "metadata": {
        "id": "g9E97mIR8LYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save inverted index to SQLite\n",
        "index_db = InvertedIndex(index_db_path)\n",
        "index_db.save_index(inverted_index)\n",
        "index_db.close_connection()"
      ],
      "metadata": {
        "id": "DUvLxLhD8OJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document mapping\n",
        "document_mapping = {documentID: Path(document_path).name for documentID, document_path in enumerate(document_paths)}"
      ],
      "metadata": {
        "id": "7gLKEvqJ8RSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SearchEngine\n",
        "search_engine = SearchEngine(index_db_path, document_mapping)"
      ],
      "metadata": {
        "id": "EC6XR5oK8VXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UbCwUxC4Elw"
      },
      "outputs": [],
      "source": [
        "# Search\n",
        "search_engine.search('HateD')\n",
        "print('\\n')\n",
        "search_engine.search('HateD Applied')\n",
        "print('\\n')\n",
        "search_engine.search(\"Can't\")\n",
        "print('\\n')\n",
        "search_engine.search(\"didn't\")\n",
        "print('\\n')\n",
        "search_engine.search(\"state-of-the-art\")\n",
        "print('\\n')\n",
        "search_engine.search(\"Elliott-Fisher\")\n",
        "print('\\n')\n",
        "search_engine.search(\"Mr.\")\n",
        "print('\\n')\n",
        "search_engine.search(\"Mr\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, NLTK's word_tokenize function splits contractions like \"Can't\" into two separate tokens: \"ca\" and \"n't\"."
      ],
      "metadata": {
        "id": "UETW-RPJBx0L"
      }
    }
  ]
}
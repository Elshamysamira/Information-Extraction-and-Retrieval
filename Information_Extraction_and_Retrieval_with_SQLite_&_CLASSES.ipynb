{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZY8S4uYl/YKdRl2IcWhFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elshamysamira/Information-Extraction-and-Retrieval/blob/nasti/Information_Extraction_and_Retrieval_with_SQLite_%26_CLASSES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Version with SQLite and Classes**"
      ],
      "metadata": {
        "id": "-kZ2kjE48rih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install chardet\n",
        "import nltk\n",
        "import chardet\n",
        "nltk.download('punkt')  # This downloads necessary datasets for tokenization\n",
        "##\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sqlite3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAD9_lAC4TM3",
        "outputId": "4b6548d4-7e15-41db-fcb1-82de7281bf11"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEFhNtX24btr",
        "outputId": "4bdc543d-1a3f-4e50-ce93-b686aa95e10b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndex:\n",
        "    def __init__(self, db_path):\n",
        "        self.db_path = db_path\n",
        "        self.conn = sqlite3.connect(self.db_path)\n",
        "        self.cursor = self.conn.cursor()\n",
        "        self.create_table()\n",
        "\n",
        "    def create_table(self):\n",
        "        self.cursor.execute('''CREATE TABLE IF NOT EXISTS inverted_index (\n",
        "                                word TEXT PRIMARY KEY,\n",
        "                                document_ids TEXT\n",
        "                            )''')\n",
        "\n",
        "    def save_index(self, inverted_index):\n",
        "        for word, document_ids in inverted_index.items():\n",
        "            doc_ids_str = ','.join(str(doc_id) for doc_id in document_ids)\n",
        "            self.cursor.execute(\"INSERT INTO inverted_index (word, document_ids) VALUES (?, ?)\", (word, doc_ids_str))\n",
        "        self.conn.commit()\n",
        "\n",
        "    def close_connection(self):\n",
        "        self.conn.close()"
      ],
      "metadata": {
        "id": "WosMhu9e4fqP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentTokenizer:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "\n",
        "    def tokenize(self):\n",
        "        tokenized_docs = {}\n",
        "        for documentID, document_path in enumerate(self.documents):\n",
        "            try:\n",
        "                with open(document_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    document_content = file.read()\n",
        "\n",
        "                tokens = word_tokenize(document_content)\n",
        "                tokenized_docs[documentID] = tokens\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "        return tokenized_docs\n"
      ],
      "metadata": {
        "id": "f2iCHrAD4lKC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndexBuilder:\n",
        "    def __init__(self, documents):\n",
        "        self.documents = documents\n",
        "        self.inverted_index = defaultdict(set)\n",
        "\n",
        "    def build_index(self):\n",
        "        for documentID, document_path in enumerate(self.documents):\n",
        "            try:\n",
        "                with open(document_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                    document_content = file.read()\n",
        "\n",
        "                for word in document_content.lower().split():\n",
        "                    self.inverted_index[word].add(documentID)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "    def get_index(self):\n",
        "        return self.inverted_index"
      ],
      "metadata": {
        "id": "JrO4ALSw4our"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SearchEngine:\n",
        "    def __init__(self, index_db_path, documents_mapping):\n",
        "        self.index_db_path = index_db_path\n",
        "        self.documents_mapping = documents_mapping\n",
        "\n",
        "    def lookup_word(self, word):\n",
        "        conn = sqlite3.connect(self.index_db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT document_ids FROM inverted_index WHERE word=?\", (word,))\n",
        "        result = cursor.fetchone()\n",
        "        conn.close()\n",
        "\n",
        "        if result:\n",
        "            document_ids = set(map(int, result[0].split(',')))\n",
        "            return document_ids\n",
        "        else:\n",
        "            return set()\n",
        "\n",
        "    def search(self, query):\n",
        "        # Tokenize the query\n",
        "        query_tokens = word_tokenize(query.lower())\n",
        "        print(f\"Tokenized Query: {query_tokens}\")  # Print out tokenized query\n",
        "\n",
        "        # Look up each token in the inverted index\n",
        "        document_sets = [self.lookup_word(token) for token in query_tokens]\n",
        "        common_documents = set.intersection(*document_sets) if document_sets else set()\n",
        "\n",
        "        if common_documents:\n",
        "            print(f\"Congratulations! The word(s) '{query}' appear together in the following document ID(s): {common_documents}\")\n",
        "            for doc_id in common_documents:\n",
        "                print(f\"Document ID: {doc_id}, Document Name: {self.documents_mapping.get(doc_id, 'Unknown')}\")\n",
        "        else:\n",
        "            print(f\"I'm sorry, the word(s) '{query}' do not appear together in any document.\")\n"
      ],
      "metadata": {
        "id": "I4Eymjv04tuv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DocumentManager:\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "\n",
        "    def get_document_paths(self):\n",
        "        return [os.path.join(self.folder_path, file) for file in os.listdir(self.folder_path)]"
      ],
      "metadata": {
        "id": "9-VOdHWg4wab"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/Documents'\n",
        "index_db_path = '/content/drive/My Drive/Documents/inverted_index.db'\n",
        "\n",
        "# Initialize DocumentManager\n",
        "doc_manager = DocumentManager(folder_path)\n",
        "document_paths = doc_manager.get_document_paths()"
      ],
      "metadata": {
        "id": "ujnxAyfn78TY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize documents\n",
        "doc_tokenizer = DocumentTokenizer(document_paths)\n",
        "tokenized_docs = doc_tokenizer.tokenize()"
      ],
      "metadata": {
        "id": "0AO-fxlf8AYG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenized documents\n",
        "output_directory = '/content/drive/My Drive/Documents/tokenized'\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "for doc_id, tokens in tokenized_docs.items():\n",
        "    output_file_path = os.path.join(output_directory, f\"tokenized_document_{doc_id}.txt\")\n",
        "    with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "        output_file.write(' '.join(tokens))"
      ],
      "metadata": {
        "id": "9usR7FEQ8EFX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build inverted index\n",
        "index_builder = InvertedIndexBuilder(document_paths)\n",
        "index_builder.build_index()\n",
        "inverted_index = index_builder.get_index()"
      ],
      "metadata": {
        "id": "g9E97mIR8LYS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save inverted index to SQLite\n",
        "index_db = InvertedIndex(index_db_path)\n",
        "index_db.save_index(inverted_index)\n",
        "index_db.close_connection()"
      ],
      "metadata": {
        "id": "DUvLxLhD8OJw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document mapping\n",
        "document_mapping = {documentID: Path(document_path).name for documentID, document_path in enumerate(document_paths)}"
      ],
      "metadata": {
        "id": "7gLKEvqJ8RSm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SearchEngine\n",
        "search_engine = SearchEngine(index_db_path, document_mapping)"
      ],
      "metadata": {
        "id": "EC6XR5oK8VXU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UbCwUxC4Elw",
        "outputId": "126d5fa9-4998-481a-d177-e33b9b96b837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Query: ['hated']\n",
            "Congratulations! The word(s) 'HateD' appear together in the following document ID(s): {1, 4, 13}\n",
            "Document ID: 1, Document Name: Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Document ID: 4, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 13, Document Name: Fifty years in Wall Street by Henry Clews.txt\n",
            "\n",
            "\n",
            "Tokenized Query: ['hated', 'applied']\n",
            "Congratulations! The word(s) 'HateD Applied' appear together in the following document ID(s): {4, 13}\n",
            "Document ID: 4, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 13, Document Name: Fifty years in Wall Street by Henry Clews.txt\n"
          ]
        }
      ],
      "source": [
        "# Search\n",
        "search_engine.search('HateD')\n",
        "print('\\n')\n",
        "search_engine.search('HateD Applied')\n"
      ]
    }
  ]
}
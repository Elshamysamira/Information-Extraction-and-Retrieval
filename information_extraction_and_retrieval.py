# -*- coding: utf-8 -*-
"""Information Extraction and Retrieval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P8OrpC5enpR3LbE6SLjS1I3xOoL07hTL
"""

!pip install nltk
!pip install chardet

import nltk
import chardet
nltk.download('punkt')  # This downloads necessary datasets for tokenization
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import defaultdict

# words = 'Hello there, my name is Samira.'

# word_tokenize(words)

from pathlib import Path
import os
from google.colab import drive
drive.mount('/content/drive/')

folder_path = '/content/drive/My Drive/Documents'
files = os.listdir(folder_path)

def printing_of_file(filepath):
    for file in files:
        print(file)

printing_of_file(files)

all_books = [Path(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]

#file_path = '/content/drive/MyDrive/Documents/Business Administration- Theory, Practice and Application.txt'  # Replace 'example.txt' with the path to your file
#with open(file_path, 'r') as f:
#    txxt = f.read()

#print(txxt)

"""**TOKENIZE FIRST**"""

######TOKENIZE HERE

def tokenize_and_save_documents(documents, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)  # Create the output directory if it doesn't exist

    tokenized_docs = {}
    for documentID, document_path in enumerate(documents):
        # Open and read the document content
        with open(document_path, 'r', encoding='utf-8') as file:
            document_content = file.read()

        # Tokenize the document content
        tokens = word_tokenize(document_content)
        tokenized_docs[documentID] = tokens

        # Save the tokenized content to a file
        output_file_path = os.path.join(output_dir, f"tokenized_document_{documentID}.txt")
        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            output_file.write(' '.join(tokens))  # Write tokens separated by spaces

    return tokenized_docs

#all_books = [Path(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]  # List of file paths to your documents
output_directory = '/content/drive/My Drive/Documents'

# Tokenize documents and save the results
tokenized_books = tokenize_and_save_documents(all_books, output_directory)
print("Tokenization complete and files saved.")

"""# FROM HERE I DON'T GUARANTEE ANYTHING ðŸ¥²"""

### TEST CODE ###

tokenized_books = [os.path.join('/content/drive/My Drive/Documents', f'document_{i}.txt') for i in range(19)]
#inverted_index = {}

#for documentID, document_path in enumerate(tokenized_books):
#    with open(document_path, 'r', encoding='utf-8') as file:
#        document_content = file.read()

#    for word in document_content.split():  # Split document content into words
#        print(documentID, os.path.basename(document_path), word)  # Print document ID, document name, and word
#        if word not in inverted_index:
#            inverted_index[word] = []
#        inverted_index[word].append(documentID)  # Append document ID to the inverted index for the word

### INVERTED INDEXING & PICKLING ###

from collections import defaultdict
import os
import pickle

# Prepare list of document file paths
tokenized_books = [os.path.join('/content/drive/My Drive/Documents', f'tokenized_document_{i}.txt') for i in range(19)]

# Initialize the inverted index as a defaultdict of sets
inverted_index = defaultdict(set)

for documentID, document_path in enumerate(tokenized_books):
    with open(document_path, 'r', encoding='utf-8') as file:
        document_content = file.read()

    # Tokenize by splitting on whitespace and standardize to lowercase
    for word in document_content.lower().split():
        print(documentID, os.path.basename(document_path), word)
        inverted_index[word].add(documentID)  # Use set to avoid duplicate document IDs

# Convert sets to lists if a list representation is preferred
for word in inverted_index:
    inverted_index[word] = list(inverted_index[word])

# Save the inverted index using pickle
with open('/content/drive/My Drive/Documents/inverted_index.pkl', 'wb') as f:
    pickle.dump(inverted_index, f)

print("Inverted index has been saved successfully.")

# Assuming all_books is a list of Path objects
document_mapping = {documentID: document.name for documentID, document in enumerate(all_books)}

def lookup_word(word):
    if word in inverted_index:
        document_ids = inverted_index[word]
        print(f"Congratulations! I could find the word '{word}' in the following document ID(s): {document_ids}")
        for doc_id in document_ids:
            print(f"Document ID: {doc_id}, Document Name: {document_mapping[doc_id]}")
    else:
        print(f"I'm sorry, I couldn't find the word '{word}' in any document.")

lookup_word('supply')
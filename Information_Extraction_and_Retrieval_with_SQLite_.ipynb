{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO4nKVDxDd9SYhZshqs+QCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elshamysamira/Information-Extraction-and-Retrieval/blob/nasti/Information_Extraction_and_Retrieval_with_SQLite_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R5G2jEM-IpG",
        "outputId": "ed6eff9a-e4d3-41bf-8961-10c5e8d4001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install chardet\n",
        "import nltk\n",
        "import chardet\n",
        "nltk.download('punkt')  # This downloads necessary datasets for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwTezjx_AK4b",
        "outputId": "b7d49840-6b18-49e3-df50-45c033b8f339"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to SQLite database\n",
        "conn = sqlite3.connect('/content/drive/My Drive/Documents/inverted_index.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create table for inverted index\n",
        "cursor.execute('''CREATE TABLE IF NOT EXISTS inverted_index (\n",
        "                    word TEXT PRIMARY KEY,\n",
        "                    document_ids TEXT\n",
        "                )''')\n",
        "\n",
        "def printing_of_file(files):\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "folder_path = '/content/drive/My Drive/Documents'\n",
        "files = os.listdir(folder_path)\n",
        "printing_of_file(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LtySctn-NvI",
        "outputId": "858c08d8-47bb-430e-a64f-b5bf700666fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A thousand ways to make money.txt\n",
            "Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Business Administration- Theory, Practice and Application.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 01 (of 10).txt\n",
            "Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Fundamentals of Prosperity- What They Are and Whence They Come by Roger Ward Babson.txt\n",
            "The Knack of Managing by Lewis K. Urquhart and Herbert Watson.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 04 (of 10).txt\n",
            "Creating Capital by Frederick L. Lipman.txt\n",
            "Twenty Years of Hus'ling by J. P. Johnston.txt\n",
            "Business Hints for Men and Women by A. R. Calhoun.txt\n",
            "The Young Man in Business by Edward William Bok.txt\n",
            "Forging Ahead in Business by Alexander Hamilton Institute.txt\n",
            "Fifty years in Wall Street by Henry Clews.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration.txt\n",
            "Increasing Human Efficiency in Business by Walter Dill Scott.txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 02 (of 10).txt\n",
            "Cyclopedia of Commerce, Accountancy, Business Administration, v. 03 (of 10).txt\n",
            "Up To Date Business by Seymour Eaton.txt\n",
            "inverted_index.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_save_documents(documents, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)  # Create the output directory if it doesn't exist\n",
        "\n",
        "    tokenized_docs = {}\n",
        "    for documentID, document_path in enumerate(documents):\n",
        "        try:\n",
        "            # Open and read the document content\n",
        "            with open(document_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                document_content = file.read()\n",
        "\n",
        "            # Tokenize the document content\n",
        "            tokens = word_tokenize(document_content)\n",
        "            tokenized_docs[documentID] = tokens\n",
        "\n",
        "            # Save the tokenized content to a file\n",
        "            output_file_path = os.path.join(output_dir, f\"tokenized_document_{documentID}.txt\")\n",
        "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "                output_file.write(' '.join(tokens))  # Write tokens separated by spaces\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "    return tokenized_docs\n"
      ],
      "metadata": {
        "id": "PQ_nezxA-cvW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize documents and save the results\n",
        "tokenized_books = tokenize_and_save_documents(all_books, output_directory)\n",
        "print(\"Tokenization complete and files saved.\")\n",
        "\n",
        "# Initialize the inverted index as a defaultdict of sets\n",
        "inverted_index = defaultdict(set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88rzXXnw-dmU",
        "outputId": "3b45d3f9-ce96-45ed-fbe5-c7e71da39f1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete and files saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for documentID, document_path in enumerate(all_books):\n",
        "    try:\n",
        "        with open(document_path, 'r', encoding='latin-1') as file:\n",
        "            document_content = file.read()\n",
        "\n",
        "        # Tokenize by splitting on whitespace and standardize to lowercase\n",
        "        for word in document_content.lower().split():\n",
        "            inverted_index[word].add(documentID)  # Use set to avoid duplicate document IDs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing document {document_path}: {e}\")\n",
        "\n",
        "# Convert sets to lists for storing in SQLite database\n",
        "for word in inverted_index:\n",
        "    document_ids = ','.join(map(str, inverted_index[word]))  # Convert set to comma-separated string\n",
        "    cursor.execute(\"INSERT INTO inverted_index (word, document_ids) VALUES (?, ?)\", (word, document_ids))\n",
        "\n",
        "# Commit changes and close connection\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"Inverted index has been saved successfully in SQLite database.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27hzLya3-gbC",
        "outputId": "833c8a4c-a511-497d-df96-f7fa7786f7d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inverted index has been saved successfully in SQLite database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_books is a list of Path objects\n",
        "document_mapping = {documentID: document.name for documentID, document in enumerate(all_books)}\n",
        "\n",
        "def lookup_word(word):\n",
        "    # Tokenize the query word\n",
        "    query_tokens = word_tokenize(word.lower())\n",
        "\n",
        "    # Print out the tokens\n",
        "    print(\"Query Tokens:\", query_tokens)\n",
        "\n",
        "    conn = sqlite3.connect('/content/drive/My Drive/Documents/inverted_index.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Initialize a list to store sets of document IDs where each token appears\n",
        "    document_sets = []\n",
        "\n",
        "    # Iterate over each token in the query word\n",
        "    for token in query_tokens:\n",
        "        # Search for the token in the inverted index\n",
        "        cursor.execute(\"SELECT document_ids FROM inverted_index WHERE word=?\", (token,))\n",
        "        result = cursor.fetchone()\n",
        "\n",
        "        # If the token is found in the inverted index, add the document IDs to the list\n",
        "        if result:\n",
        "            document_sets.append(set(map(int, result[0].split(','))))\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "    # Check if any document sets were found for the query tokens\n",
        "    if document_sets:\n",
        "        # Find the intersection of all document sets\n",
        "        common_documents = set.intersection(*document_sets)\n",
        "\n",
        "        if common_documents:\n",
        "            print(f\"Congratulations! The word(s) '{word}' appear together in the following document ID(s): {common_documents}\")\n",
        "            for doc_id in common_documents:\n",
        "                print(f\"Document ID: {doc_id}, Document Name: {document_mapping[doc_id]}\")\n",
        "        else:\n",
        "            print(f\"I'm sorry, the word(s) '{word}' do not appear together in any document.\")\n",
        "    else:\n",
        "        print(f\"I'm sorry, I couldn't find any of the word(s) '{word}' in any document.\")\n",
        "\n",
        "\n",
        "lookup_word('HateD')  # Example query with multiple tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIOixvRG-nrk",
        "outputId": "407d1231-bd2a-4723-cc87-ec24a72d11e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Tokens: ['hated']\n",
            "Congratulations! The word(s) 'HateD' appear together in the following document ID(s): {1, 4, 13}\n",
            "Document ID: 1, Document Name: Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Document ID: 4, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 13, Document Name: Fifty years in Wall Street by Henry Clews.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word('HateD Applied')  # Example query with multiple tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSGDPfbXDEMZ",
        "outputId": "e3bff848-880a-43df-c220-117f8ba532f2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Tokens: ['hated', 'applied']\n",
            "Congratulations! The word(s) 'HateD Applied' appear together in the following document ID(s): {4, 13}\n",
            "Document ID: 4, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 13, Document Name: Fifty years in Wall Street by Henry Clews.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can look up for several words that both appear together (not near to each other) in a document.\n",
        "\n",
        "\n",
        "But there are still some issues like this (what if the user will use them in the query?):"
      ],
      "metadata": {
        "id": "ed9iMMslKDOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word(\" Can't\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbH607PmKUNo",
        "outputId": "22b20a72-1b4e-448e-cb66-d15cfec5f4e8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Tokens: ['ca', \"n't\"]\n",
            "I'm sorry, I couldn't find any of the word(s) ' Can't' in any document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word(\"Cannot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu7l8MyEKf5l",
        "outputId": "a57d1cf0-6b98-4119-e0b3-665ff05da261"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Tokens: ['can', 'not']\n",
            "Congratulations! The word(s) ' Cannot' appear together in the following document ID(s): {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n",
            "Document ID: 0, Document Name: A thousand ways to make money.txt\n",
            "Document ID: 1, Document Name: Dumbells of Business by Louis Custer Martin Reed.txt\n",
            "Document ID: 2, Document Name: Business Administration- Theory, Practice and Application.txt\n",
            "Document ID: 3, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 01 (of 10).txt\n",
            "Document ID: 4, Document Name: Confessions of a Tradesman by Frank Thomas Bullen.txt\n",
            "Document ID: 5, Document Name: Fundamentals of Prosperity- What They Are and Whence They Come by Roger Ward Babson.txt\n",
            "Document ID: 6, Document Name: The Knack of Managing by Lewis K. Urquhart and Herbert Watson.txt\n",
            "Document ID: 7, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 04 (of 10).txt\n",
            "Document ID: 8, Document Name: Creating Capital by Frederick L. Lipman.txt\n",
            "Document ID: 9, Document Name: Twenty Years of Hus'ling by J. P. Johnston.txt\n",
            "Document ID: 10, Document Name: Business Hints for Men and Women by A. R. Calhoun.txt\n",
            "Document ID: 11, Document Name: The Young Man in Business by Edward William Bok.txt\n",
            "Document ID: 12, Document Name: Forging Ahead in Business by Alexander Hamilton Institute.txt\n",
            "Document ID: 13, Document Name: Fifty years in Wall Street by Henry Clews.txt\n",
            "Document ID: 14, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration.txt\n",
            "Document ID: 15, Document Name: Increasing Human Efficiency in Business by Walter Dill Scott.txt\n",
            "Document ID: 16, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 02 (of 10).txt\n",
            "Document ID: 17, Document Name: Cyclopedia of Commerce, Accountancy, Business Administration, v. 03 (of 10).txt\n",
            "Document ID: 18, Document Name: Up To Date Business by Seymour Eaton.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word(\"Don't, didn't\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j05JwMRDKkCf",
        "outputId": "f84923a8-4ad5-49cb-ae8b-744e1c31d6b3"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Tokens: ['do', \"n't\", ',', 'did', \"n't\"]\n",
            "Congratulations! The word(s) 'Don't, didn't' appear together in the following document ID(s): {15}\n",
            "Document ID: 15, Document Name: Increasing Human Efficiency in Business by Walter Dill Scott.txt\n"
          ]
        }
      ]
    }
  ]
}
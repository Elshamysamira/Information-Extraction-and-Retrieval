{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Elshamysamira/Information-Extraction-and-Retrieval/blob/main/Information_Extraction_and_Retrieval.ipynb",
      "authorship_tag": "ABX9TyO65AavHpQfuo98rTyhR4nV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elshamysamira/Information-Extraction-and-Retrieval/blob/sami/Information_Extraction_and_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install chardet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDSYs0Vm7D_3",
        "outputId": "fd40e0eb-afdf-4663-fe54-87290d830a40"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "import chardet\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "kqGWMCxR6Ufg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # This downloads necessary datasets for tokenization"
      ],
      "metadata": {
        "id": "S3c_RbXjfnwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "yZMJ0xkC3eQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea61c936-93da-4bb8-9ec5-70a57c252514"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/Documents'\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "def printing_of_file(filepath):\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "printing_of_file(files)"
      ],
      "metadata": {
        "id": "ta-LWrDI4cVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plain_text_books = [Path(os.path.join(folder_path, f)) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]"
      ],
      "metadata": {
        "id": "f2VrhmfNDZfs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "KXajX789d_O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######TOKENIZE HERE"
      ],
      "metadata": {
        "id": "SJTBwOeGeHFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_save_documents(documents, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)  # Create the output directory if it doesn't exist\n",
        "\n",
        "    tokenized_docs = {}\n",
        "    for documentID, document_path in enumerate(documents):\n",
        "        # Open and read the document content\n",
        "        with open(document_path, 'r', encoding='utf-8') as file:\n",
        "            document_content = file.read()\n",
        "\n",
        "        # Tokenize the document content\n",
        "        tokens = word_tokenize(document_content)\n",
        "        tokenized_docs[documentID] = tokens\n",
        "\n",
        "        # Save the tokenized content to a file\n",
        "        output_file_path = os.path.join(output_dir, f\"tokenized_document_{documentID}.txt\")\n",
        "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "            output_file.write(' '.join(tokens))  # Write tokens separated by spaces\n",
        "\n",
        "    return tokenized_docs\n"
      ],
      "metadata": {
        "id": "hVAzCxJ1T9El"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_directory = '/content/drive/My Drive/Documents'\n",
        "\n",
        "# Tokenize documents and save the results\n",
        "tokenized_books = [os.path.join('/content/drive/My Drive/Documents', f'tokenized_document_{i}.txt') for i in range(len(tokenize_and_save_documents(all_books, output_directory)))]\n",
        "\n",
        "print(\"Tokenization complete and files saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKMIUKeFUKXa",
        "outputId": "611e0157-7ff8-471f-a7e8-edd52a6ef8cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization complete and files saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index and Pickling"
      ],
      "metadata": {
        "id": "RV5dcOQwXjyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the inverted index as a defaultdict of sets\n",
        "inverted_index = defaultdict(set)\n",
        "\n",
        "for documentID, document_path in enumerate(tokenized_books):\n",
        "    with open(document_path, 'r', encoding='utf-8') as file:\n",
        "        document_content = file.read()\n",
        "\n",
        "    # Tokenize by splitting on whitespace and standardize to lowercase\n",
        "    for word in document_content.lower().split():\n",
        "        print(documentID, os.path.basename(document_path), word)\n",
        "        inverted_index[word].add(documentID)  # Use set to avoid duplicate document IDs\n",
        "\n",
        "# Convert sets to lists if a list representation is preferred\n",
        "    for word in inverted_index:\n",
        "        inverted_index[word] = list(inverted_index[word])\n",
        "\n",
        "# Save the inverted index using pickle\n",
        "    with open('/content/drive/My Drive/Documents/inverted_index.pkl', 'wb') as f:\n",
        "        pickle.dump(inverted_index, f)\n",
        "\n",
        "#print(\"Inverted index has been saved successfully.\")\n"
      ],
      "metadata": {
        "id": "srcZeqTaJcT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce97f1c2-9eff-4e4b-8a9e-15462b19a29f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt project\n",
            "0 tokenized_document_0.txt gutenberg\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt theory\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt practice\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt application\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt vol\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt 1\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt use\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt anyone\n",
            "0 tokenized_document_0.txt anywhere\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt united\n",
            "0 tokenized_document_0.txt states\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt parts\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt no\n",
            "0 tokenized_document_0.txt cost\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt almost\n",
            "0 tokenized_document_0.txt no\n",
            "0 tokenized_document_0.txt restrictions\n",
            "0 tokenized_document_0.txt whatsoever\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt may\n",
            "0 tokenized_document_0.txt copy\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt give\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt away\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt re-use\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt under\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt terms\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt project\n",
            "0 tokenized_document_0.txt gutenberg\n",
            "0 tokenized_document_0.txt license\n",
            "0 tokenized_document_0.txt included\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt online\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt www.gutenberg.org\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt located\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt united\n",
            "0 tokenized_document_0.txt states\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt check\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt laws\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt country\n",
            "0 tokenized_document_0.txt where\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt located\n",
            "0 tokenized_document_0.txt before\n",
            "0 tokenized_document_0.txt using\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt title\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt theory\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt practice\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt application\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt vol\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt 1\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt contributor\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt oscar\n",
            "0 tokenized_document_0.txt p.\n",
            "0 tokenized_document_0.txt austin\n",
            "0 tokenized_document_0.txt ernest\n",
            "0 tokenized_document_0.txt l.\n",
            "0 tokenized_document_0.txt bogart\n",
            "0 tokenized_document_0.txt editor\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt samuel\n",
            "0 tokenized_document_0.txt macclintock\n",
            "0 tokenized_document_0.txt walter\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt moody\n",
            "0 tokenized_document_0.txt release\n",
            "0 tokenized_document_0.txt date\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt november\n",
            "0 tokenized_document_0.txt 21\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt 2017\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt #\n",
            "0 tokenized_document_0.txt 56018\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt recently\n",
            "0 tokenized_document_0.txt updated\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt june\n",
            "0 tokenized_document_0.txt 14\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt 2020\n",
            "0 tokenized_document_0.txt language\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt english\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt start\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt project\n",
            "0 tokenized_document_0.txt gutenberg\n",
            "0 tokenized_document_0.txt ebook\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt theory\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt practice\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt application\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt vol\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt 1\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt e-text\n",
            "0 tokenized_document_0.txt prepared\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt juliet\n",
            "0 tokenized_document_0.txt sutherland\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt carol\n",
            "0 tokenized_document_0.txt brown\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt online\n",
            "0 tokenized_document_0.txt distributed\n",
            "0 tokenized_document_0.txt proofreading\n",
            "0 tokenized_document_0.txt team\n",
            "0 tokenized_document_0.txt (\n",
            "0 tokenized_document_0.txt http\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt //www.pgdp.net\n",
            "0 tokenized_document_0.txt )\n",
            "0 tokenized_document_0.txt note\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt project\n",
            "0 tokenized_document_0.txt gutenberg\n",
            "0 tokenized_document_0.txt also\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt html\n",
            "0 tokenized_document_0.txt version\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt file\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt includes\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt original\n",
            "0 tokenized_document_0.txt illustrations\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt see\n",
            "0 tokenized_document_0.txt 56018-h.htm\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt 56018-h.zip\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt (\n",
            "0 tokenized_document_0.txt https\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt //www.gutenberg.org/cache/epub/56018/pg56018-images.html\n",
            "0 tokenized_document_0.txt )\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt (\n",
            "0 tokenized_document_0.txt https\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt //www.gutenberg.org/files/56018/56018-h.zip\n",
            "0 tokenized_document_0.txt )\n",
            "0 tokenized_document_0.txt transcriber\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt note\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt words\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt phrases\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt italics\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt surrounded\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt underscores\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt _like\n",
            "0 tokenized_document_0.txt this_\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt subscripted\n",
            "0 tokenized_document_0.txt numbers\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt chemical\n",
            "0 tokenized_document_0.txt formulas\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt page\n",
            "0 tokenized_document_0.txt 347\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt presented\n",
            "0 tokenized_document_0.txt within\n",
            "0 tokenized_document_0.txt curly\n",
            "0 tokenized_document_0.txt braces\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt preceded\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt underscore\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt example\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt c_\n",
            "0 tokenized_document_0.txt {\n",
            "0 tokenized_document_0.txt 6\n",
            "0 tokenized_document_0.txt }\n",
            "0 tokenized_document_0.txt h_\n",
            "0 tokenized_document_0.txt {\n",
            "0 tokenized_document_0.txt 3\n",
            "0 tokenized_document_0.txt }\n",
            "0 tokenized_document_0.txt ohoch_\n",
            "0 tokenized_document_0.txt {\n",
            "0 tokenized_document_0.txt 3\n",
            "0 tokenized_document_0.txt }\n",
            "0 tokenized_document_0.txt cho\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt page\n",
            "0 tokenized_document_0.txt numbers\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt displayed\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt right\n",
            "0 tokenized_document_0.txt margin\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt text\n",
            "0 tokenized_document_0.txt books\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt organization\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt management\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt advertising\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt salesmanship\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt trade\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt transportation\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt money\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt banking\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt insurance\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt investments\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt speculation\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt accounting\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt auditing\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt cost\n",
            "0 tokenized_document_0.txt accounting\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt law\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt legal\n",
            "0 tokenized_document_0.txt forms\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt *\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt theory\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt practice\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt application\n",
            "0 tokenized_document_0.txt editor-in-chief\n",
            "0 tokenized_document_0.txt walter\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt moody\n",
            "0 tokenized_document_0.txt general\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt chicago\n",
            "0 tokenized_document_0.txt association\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt author\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt sell\n",
            "0 tokenized_document_0.txt things.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt managing\n",
            "0 tokenized_document_0.txt editor\n",
            "0 tokenized_document_0.txt samuel\n",
            "0 tokenized_document_0.txt macclintock\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ph\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt editorial\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt educational\n",
            "0 tokenized_document_0.txt director\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt la\n",
            "0 tokenized_document_0.txt salle\n",
            "0 tokenized_document_0.txt extension\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt especially\n",
            "0 tokenized_document_0.txt designed\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt meet\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt practical\n",
            "0 tokenized_document_0.txt every-day\n",
            "0 tokenized_document_0.txt needs\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt active\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt contains\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt fundamental\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt basic\n",
            "0 tokenized_document_0.txt principles\n",
            "0 tokenized_document_0.txt upon\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt founded\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt conducted\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt maintained\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt those\n",
            "0 tokenized_document_0.txt looking\n",
            "0 tokenized_document_0.txt forward\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt career\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt forms\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt basis\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt practical\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt systematic\n",
            "0 tokenized_document_0.txt course\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt published\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt la\n",
            "0 tokenized_document_0.txt salle\n",
            "0 tokenized_document_0.txt extension\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt chicago\n",
            "0 tokenized_document_0.txt copyright\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt 1910\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt lasalle\n",
            "0 tokenized_document_0.txt extension\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt ¶\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt treatise\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt especially\n",
            "0 tokenized_document_0.txt prepared\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt e.\n",
            "0 tokenized_document_0.txt l.\n",
            "0 tokenized_document_0.txt bogart\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ph\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt associate\n",
            "0 tokenized_document_0.txt professor\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt economics\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt illinois\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt author\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt economic\n",
            "0 tokenized_document_0.txt history\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt united\n",
            "0 tokenized_document_0.txt states\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt hon\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt o.\n",
            "0 tokenized_document_0.txt p.\n",
            "0 tokenized_document_0.txt austin\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt chief\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt bureau\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt statistics\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt department\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt labor\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt john\n",
            "0 tokenized_document_0.txt bascom\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ll\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt former\n",
            "0 tokenized_document_0.txt president\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt wisconsin\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt supplemented\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt writings\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt recognized\n",
            "0 tokenized_document_0.txt experts\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt production\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt preservation\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt distribution\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt wealth\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt treatment\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt modern\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt popular\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt authoritative\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt volume\n",
            "0 tokenized_document_0.txt contains\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt timely\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt practical\n",
            "0 tokenized_document_0.txt suggestions\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt applied\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt profit\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt any\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt also\n",
            "0 tokenized_document_0.txt arranged\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt serve\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt quick\n",
            "0 tokenized_document_0.txt reference\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt includes\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt complete\n",
            "0 tokenized_document_0.txt table\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt contents\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt comprehensive\n",
            "0 tokenized_document_0.txt index\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt test\n",
            "0 tokenized_document_0.txt questions\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt walter\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt moody\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt editor-in-chief\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt introduction\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt walter\n",
            "0 tokenized_document_0.txt d.\n",
            "0 tokenized_document_0.txt moody\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt general\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt chicago\n",
            "0 tokenized_document_0.txt association\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt author\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt sell\n",
            "0 tokenized_document_0.txt things.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt _the\n",
            "0 tokenized_document_0.txt recipe\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt perpetual\n",
            "0 tokenized_document_0.txt ignorance\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt satisfied\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt your\n",
            "0 tokenized_document_0.txt own\n",
            "0 tokenized_document_0.txt opinion\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt content\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt your\n",
            "0 tokenized_document_0.txt knowledge._\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt contest\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt wits\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt era\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt commercial\n",
            "0 tokenized_document_0.txt activity\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt ever\n",
            "0 tokenized_document_0.txt known\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt development\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt marvels\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt century\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt few\n",
            "0 tokenized_document_0.txt years\n",
            "0 tokenized_document_0.txt ago\n",
            "0 tokenized_document_0.txt science\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt factor\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt little\n",
            "0 tokenized_document_0.txt known\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt less\n",
            "0 tokenized_document_0.txt appreciated\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt amazing\n",
            "0 tokenized_document_0.txt advantages\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt intellectual\n",
            "0 tokenized_document_0.txt attainments\n",
            "0 tokenized_document_0.txt were\n",
            "0 tokenized_document_0.txt utterly\n",
            "0 tokenized_document_0.txt without\n",
            "0 tokenized_document_0.txt recognition\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt however\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt become\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt contest\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt quickest\n",
            "0 tokenized_document_0.txt perception\n",
            "0 tokenized_document_0.txt wins\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt thus\n",
            "0 tokenized_document_0.txt transforming\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt counting\n",
            "0 tokenized_document_0.txt room\n",
            "0 tokenized_document_0.txt into\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt battle\n",
            "0 tokenized_document_0.txt ground\n",
            "0 tokenized_document_0.txt upon\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt brain\n",
            "0 tokenized_document_0.txt matches\n",
            "0 tokenized_document_0.txt brain\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt supremacy\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt success-educated\n",
            "0 tokenized_document_0.txt enthusiasm\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt ah\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt enchanting\n",
            "0 tokenized_document_0.txt word\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt s-u-c-c-e-s-s\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt does\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt require\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt magic\n",
            "0 tokenized_document_0.txt key\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt unlock\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt door\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt efficiency\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt mystic\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt mysterious\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt applied\n",
            "0 tokenized_document_0.txt method\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt really\n",
            "0 tokenized_document_0.txt resourceful\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt day\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt great\n",
            "0 tokenized_document_0.txt successes\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt marvelous\n",
            "0 tokenized_document_0.txt achievements\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt enterprise\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sum\n",
            "0 tokenized_document_0.txt total\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt contained\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt two\n",
            "0 tokenized_document_0.txt words\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt words\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt electrify\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt nevertheless\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt educated\n",
            "0 tokenized_document_0.txt enthusiasm\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt changing\n",
            "0 tokenized_document_0.txt conditions\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt opportunities\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt formidable\n",
            "0 tokenized_document_0.txt barrier\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt always\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt senseless\n",
            "0 tokenized_document_0.txt opposition\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt those\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt whom\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt benefit\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt changing\n",
            "0 tokenized_document_0.txt conditions\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt order\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt day\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt enlightenment\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt worked\n",
            "0 tokenized_document_0.txt wonders\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt olden\n",
            "0 tokenized_document_0.txt times\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt affairs\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt obliged\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt guard\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt property\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt loved\n",
            "0 tokenized_document_0.txt ones\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt building\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt moat\n",
            "0 tokenized_document_0.txt around\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt house\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt posting\n",
            "0 tokenized_document_0.txt sentinels\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt around\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt estate\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt time\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt long\n",
            "0 tokenized_document_0.txt past\n",
            "0 tokenized_document_0.txt when\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt because\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt prejudice\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt perversity\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt ignorance\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt believed\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt opportunity\n",
            "0 tokenized_document_0.txt knocked\n",
            "0 tokenized_document_0.txt only\n",
            "0 tokenized_document_0.txt once\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt any\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt door\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt thanks\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt deeper\n",
            "0 tokenized_document_0.txt insight\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt believe\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt life\n",
            "0 tokenized_document_0.txt itself\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt opportunity\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt very\n",
            "0 tokenized_document_0.txt air\n",
            "0 tokenized_document_0.txt we\n",
            "0 tokenized_document_0.txt breathe\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt opportunity\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt each\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt day\n",
            "0 tokenized_document_0.txt presents\n",
            "0 tokenized_document_0.txt broader\n",
            "0 tokenized_document_0.txt opportunities\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt accomplishing\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt because\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt directed\n",
            "0 tokenized_document_0.txt energy\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt alone\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt accepted\n",
            "0 tokenized_document_0.txt dogma\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt making\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt creed\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt doctrine\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt tenet\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt religion\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt whichever\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt may\n",
            "0 tokenized_document_0.txt care\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt term\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt great\n",
            "0 tokenized_document_0.txt captains\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt industry\n",
            "0 tokenized_document_0.txt everywhere\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt count\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt does\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt think\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt study\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt plan\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt part\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt daily\n",
            "0 tokenized_document_0.txt occupation\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt development\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt affairs\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt interested\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt newer\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt ways\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt get\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt done\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt standard\n",
            "0 tokenized_document_0.txt employed\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt lines\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt only\n",
            "0 tokenized_document_0.txt yesterday\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt genius\n",
            "0 tokenized_document_0.txt advanced\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt idea\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt found\n",
            "0 tokenized_document_0.txt himself\n",
            "0 tokenized_document_0.txt ridiculed\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt innovation\n",
            "0 tokenized_document_0.txt opposed\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt sides\n",
            "0 tokenized_document_0.txt because\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt idea\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt different\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt counts\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt trend\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt affairs\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt never\n",
            "0 tokenized_document_0.txt counted\n",
            "0 tokenized_document_0.txt before\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt keep\n",
            "0 tokenized_document_0.txt step\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt changing\n",
            "0 tokenized_document_0.txt times\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt everything\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt subjective\n",
            "0 tokenized_document_0.txt reason\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt acting\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt mighty\n",
            "0 tokenized_document_0.txt dynamic\n",
            "0 tokenized_document_0.txt force\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt changing\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt viewpoint\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt life\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt suppose\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt stroke\n",
            "0 tokenized_document_0.txt oar\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt varsity\n",
            "0 tokenized_document_0.txt crew\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt while\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt race\n",
            "0 tokenized_document_0.txt against\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt opposing\n",
            "0 tokenized_document_0.txt crew\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt competitive\n",
            "0 tokenized_document_0.txt institution\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt should\n",
            "0 tokenized_document_0.txt suddenly\n",
            "0 tokenized_document_0.txt stop\n",
            "0 tokenized_document_0.txt rowing\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt harmony\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt associates\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt begin\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt row\n",
            "0 tokenized_document_0.txt backwards-that\n",
            "0 tokenized_document_0.txt crew\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt get\n",
            "0 tokenized_document_0.txt very\n",
            "0 tokenized_document_0.txt far\n",
            "0 tokenized_document_0.txt without\n",
            "0 tokenized_document_0.txt trouble\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt suppose\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt lawn\n",
            "0 tokenized_document_0.txt mower\n",
            "0 tokenized_document_0.txt should\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt reversed\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt forced\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt run\n",
            "0 tokenized_document_0.txt backwards\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt much\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt made\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt cutting\n",
            "0 tokenized_document_0.txt grass\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt lawn\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt varsity\n",
            "0 tokenized_document_0.txt crews\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt lawn\n",
            "0 tokenized_document_0.txt mowers\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt move\n",
            "0 tokenized_document_0.txt forward\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt advance\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt times\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt great\n",
            "0 tokenized_document_0.txt merchant\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt chicago\n",
            "0 tokenized_document_0.txt tells\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt good\n",
            "0 tokenized_document_0.txt story\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt youth\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt member\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt state\n",
            "0 tokenized_document_0.txt regiment\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt militia\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt certain\n",
            "0 tokenized_document_0.txt occasion\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt company\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt sent\n",
            "0 tokenized_document_0.txt out\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt dress\n",
            "0 tokenized_document_0.txt parade\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt old\n",
            "0 tokenized_document_0.txt maiden\n",
            "0 tokenized_document_0.txt aunt\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt considerable\n",
            "0 tokenized_document_0.txt colonial\n",
            "0 tokenized_document_0.txt blood\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt her\n",
            "0 tokenized_document_0.txt veins\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt took\n",
            "0 tokenized_document_0.txt much\n",
            "0 tokenized_document_0.txt pride\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt her\n",
            "0 tokenized_document_0.txt nephew\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt company\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt while\n",
            "0 tokenized_document_0.txt reviewing\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt parade\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt she\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt suddenly\n",
            "0 tokenized_document_0.txt heard\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt exclaim\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt why\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt every\n",
            "0 tokenized_document_0.txt single\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt company\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt out\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt step\n",
            "0 tokenized_document_0.txt excepting\n",
            "0 tokenized_document_0.txt my\n",
            "0 tokenized_document_0.txt nephew.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt get\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt realize\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt lies\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt keeping\n",
            "0 tokenized_document_0.txt step\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt making\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt changing\n",
            "0 tokenized_document_0.txt conditions\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt generally\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt mistake\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt thinking\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt everything\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt out\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt harmony\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt themselves\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt worth\n",
            "0 tokenized_document_0.txt searching\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt realizes\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt newer\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt principles\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt conducting\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt value\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt also\n",
            "0 tokenized_document_0.txt knows\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt pays\n",
            "0 tokenized_document_0.txt him\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt search\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt same\n",
            "0 tokenized_document_0.txt old\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt doing\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt longer\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt successfully\n",
            "0 tokenized_document_0.txt employed\n",
            "0 tokenized_document_0.txt month\n",
            "0 tokenized_document_0.txt after\n",
            "0 tokenized_document_0.txt month\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt year\n",
            "0 tokenized_document_0.txt after\n",
            "0 tokenized_document_0.txt year\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt under\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt old\n",
            "0 tokenized_document_0.txt regime\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt modern\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt up-to-date\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt physician\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt lawyer\n",
            "0 tokenized_document_0.txt finds\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt compete\n",
            "0 tokenized_document_0.txt successfully\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt compelled\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt search\n",
            "0 tokenized_document_0.txt without\n",
            "0 tokenized_document_0.txt ceasing\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt order\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt may\n",
            "0 tokenized_document_0.txt comprehend\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt advancement\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt treatments\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt procedures\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt fails\n",
            "0 tokenized_document_0.txt belong\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt excuses.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt demand\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt trained\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt president\n",
            "0 tokenized_document_0.txt james\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt illinois\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt asked\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt any\n",
            "0 tokenized_document_0.txt demand\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt houses\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt college-bred\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt reply\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt demand\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt far\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt excess\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt supply\n",
            "0 tokenized_document_0.txt since\n",
            "0 tokenized_document_0.txt courses\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt were\n",
            "0 tokenized_document_0.txt established\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt our\n",
            "0 tokenized_document_0.txt institution\n",
            "0 tokenized_document_0.txt seven\n",
            "0 tokenized_document_0.txt years\n",
            "0 tokenized_document_0.txt ago\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt each\n",
            "0 tokenized_document_0.txt year\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt brought\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt requests\n",
            "0 tokenized_document_0.txt than\n",
            "0 tokenized_document_0.txt we\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt recommend.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt ten\n",
            "0 tokenized_document_0.txt years\n",
            "0 tokenized_document_0.txt ago\n",
            "0 tokenized_document_0.txt president\n",
            "0 tokenized_document_0.txt james\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt ridiculed\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt advancing\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt idea\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt establishment\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt school\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt commerce\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt connection\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt university\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt commercial\n",
            "0 tokenized_document_0.txt schools\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt part\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt regularly\n",
            "0 tokenized_document_0.txt established\n",
            "0 tokenized_document_0.txt courses\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt nearly\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt great\n",
            "0 tokenized_document_0.txt universities\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt our\n",
            "0 tokenized_document_0.txt country\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt trained\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt theory\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt practice\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt always\n",
            "0 tokenized_document_0.txt occupy\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt best\n",
            "0 tokenized_document_0.txt positions\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt always\n",
            "0 tokenized_document_0.txt command\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt salaries\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt value\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt emergencies\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt times\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt accomplishment\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt satisfactory\n",
            "0 tokenized_document_0.txt results\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt various\n",
            "0 tokenized_document_0.txt enterprises\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt engaged\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt without\n",
            "0 tokenized_document_0.txt being\n",
            "0 tokenized_document_0.txt able\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt give\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt explanation\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt principles\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt applied\n",
            "0 tokenized_document_0.txt successfully\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt years\n",
            "0 tokenized_document_0.txt seem\n",
            "0 tokenized_document_0.txt apparently\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt counted\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt frequently\n",
            "0 tokenized_document_0.txt evident\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt cases\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt very\n",
            "0 tokenized_document_0.txt insignificant\n",
            "0 tokenized_document_0.txt thing\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt mere\n",
            "0 tokenized_document_0.txt oversight\n",
            "0 tokenized_document_0.txt perchance\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt direct\n",
            "0 tokenized_document_0.txt cause\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt failure\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt able\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt put\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt finger\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt precise\n",
            "0 tokenized_document_0.txt cause\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt method\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt locate\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt cause\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt disaster\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt then\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt real\n",
            "0 tokenized_document_0.txt appreciation\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt fully\n",
            "0 tokenized_document_0.txt realized\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt paid\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt know\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt failure\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt often\n",
            "0 tokenized_document_0.txt chargeable\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt refusal\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt learn\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt mistakes\n",
            "0 tokenized_document_0.txt how\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt avoid\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt than\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt making\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt good\n",
            "0 tokenized_document_0.txt teacher\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt deny\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt value\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt gained\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt learning\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt others\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt we\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt same\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt same\n",
            "0 tokenized_document_0.txt view\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt similar\n",
            "0 tokenized_document_0.txt experiences\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt pathways\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt road\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt individual\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt narrow\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt rugged\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt commonly\n",
            "0 tokenized_document_0.txt accepted\n",
            "0 tokenized_document_0.txt fact\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt every\n",
            "0 tokenized_document_0.txt ten\n",
            "0 tokenized_document_0.txt dollars\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt high-salaried\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt draws\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt receives\n",
            "0 tokenized_document_0.txt nine\n",
            "0 tokenized_document_0.txt dollars\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt knows\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt dollar\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt does\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt same\n",
            "0 tokenized_document_0.txt basis\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt employing\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt large\n",
            "0 tokenized_document_0.txt force\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt realizes\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt own\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt worth\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt applied\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt affairs\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt lies\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt so\n",
            "0 tokenized_document_0.txt much\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt himself\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt how\n",
            "0 tokenized_document_0.txt much\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt encourage\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt employes\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt either\n",
            "0 tokenized_document_0.txt case\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt own\n",
            "0 tokenized_document_0.txt personal\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt power\n",
            "0 tokenized_document_0.txt behind\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt throne\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt excess\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt present\n",
            "0 tokenized_document_0.txt needs\n",
            "0 tokenized_document_0.txt necessary\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt secure\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt largest\n",
            "0 tokenized_document_0.txt net\n",
            "0 tokenized_document_0.txt return\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt individual\n",
            "0 tokenized_document_0.txt effort\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt field\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt endeavor\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt realize\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt possible\n",
            "0 tokenized_document_0.txt advantage\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt efforts\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt those\n",
            "0 tokenized_document_0.txt under\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt command\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt necessity\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt possess\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt indispensable\n",
            "0 tokenized_document_0.txt perception\n",
            "0 tokenized_document_0.txt far\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt excess\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt needs\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt moment\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt discernment\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt like\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt bank\n",
            "0 tokenized_document_0.txt account\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt soon\n",
            "0 tokenized_document_0.txt runs\n",
            "0 tokenized_document_0.txt out\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt overdrawn\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt continually\n",
            "0 tokenized_document_0.txt replenished\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt checking\n",
            "0 tokenized_document_0.txt system\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sort\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt account\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt pays\n",
            "0 tokenized_document_0.txt best\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt savings\n",
            "0 tokenized_document_0.txt account\n",
            "0 tokenized_document_0.txt system.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt simply\n",
            "0 tokenized_document_0.txt corked\n",
            "0 tokenized_document_0.txt up\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt allowed\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt accumulate\n",
            "0 tokenized_document_0.txt cobwebs\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt rust\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt avail\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sharpest\n",
            "0 tokenized_document_0.txt vinegar\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt procured\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt constantly\n",
            "0 tokenized_document_0.txt replenishing\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt old\n",
            "0 tokenized_document_0.txt stock\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt 90\n",
            "0 tokenized_document_0.txt %\n",
            "0 tokenized_document_0.txt failures\n",
            "0 tokenized_document_0.txt vs.\n",
            "0 tokenized_document_0.txt 10\n",
            "0 tokenized_document_0.txt %\n",
            "0 tokenized_document_0.txt moneymakers\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt reliable\n",
            "0 tokenized_document_0.txt statistics\n",
            "0 tokenized_document_0.txt prove\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt only\n",
            "0 tokenized_document_0.txt about\n",
            "0 tokenized_document_0.txt ten\n",
            "0 tokenized_document_0.txt per\n",
            "0 tokenized_document_0.txt cent\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt people\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt engage\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt money\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt ninety\n",
            "0 tokenized_document_0.txt per\n",
            "0 tokenized_document_0.txt cent\n",
            "0 tokenized_document_0.txt become\n",
            "0 tokenized_document_0.txt insolvent\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt actually\n",
            "0 tokenized_document_0.txt encounter\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sheriff\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt go\n",
            "0 tokenized_document_0.txt into\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt hands\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt receiver\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt nevertheless\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt succeed\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sense\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt making\n",
            "0 tokenized_document_0.txt money\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt possible\n",
            "0 tokenized_document_0.txt reason\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt anyone\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt engaging\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt accumulate\n",
            "0 tokenized_document_0.txt money\n",
            "0 tokenized_document_0.txt ?\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt failures\n",
            "0 tokenized_document_0.txt due\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt intellectual\n",
            "0 tokenized_document_0.txt capacity\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt why\n",
            "0 tokenized_document_0.txt do\n",
            "0 tokenized_document_0.txt so\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt ?\n",
            "0 tokenized_document_0.txt ask\n",
            "0 tokenized_document_0.txt any\n",
            "0 tokenized_document_0.txt credit\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt tell\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt because\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt capital\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt material\n",
            "0 tokenized_document_0.txt resources\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt due\n",
            "0 tokenized_document_0.txt primarily\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt intellectual\n",
            "0 tokenized_document_0.txt capacity\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sort\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt brains\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt dig\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt sweat\n",
            "0 tokenized_document_0.txt until\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt find\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt accomplish\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt brains\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt go\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt bottom\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt brains\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt always\n",
            "0 tokenized_document_0.txt looking\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt results\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt brains\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt never\n",
            "0 tokenized_document_0.txt abandon\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt problem\n",
            "0 tokenized_document_0.txt until\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt found\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt solve\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt friend\n",
            "0 tokenized_document_0.txt once\n",
            "0 tokenized_document_0.txt told\n",
            "0 tokenized_document_0.txt me\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt inquired\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt house\n",
            "0 tokenized_document_0.txt employing\n",
            "0 tokenized_document_0.txt some\n",
            "0 tokenized_document_0.txt three\n",
            "0 tokenized_document_0.txt hundred\n",
            "0 tokenized_document_0.txt traveling\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt how\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt salesmen\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt had\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt replied\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt three.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt my\n",
            "0 tokenized_document_0.txt friend\n",
            "0 tokenized_document_0.txt asked\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt how\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt ?\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt am\n",
            "0 tokenized_document_0.txt told\n",
            "0 tokenized_document_0.txt your\n",
            "0 tokenized_document_0.txt force\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt traveling\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt numbers\n",
            "0 tokenized_document_0.txt nearly\n",
            "0 tokenized_document_0.txt three\n",
            "0 tokenized_document_0.txt hundred.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt ah\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt quite\n",
            "0 tokenized_document_0.txt different\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt replied\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt we\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt two\n",
            "0 tokenized_document_0.txt hundred\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt ninety-seven\n",
            "0 tokenized_document_0.txt traveling\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt only\n",
            "0 tokenized_document_0.txt three\n",
            "0 tokenized_document_0.txt salesmen.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt quite\n",
            "0 tokenized_document_0.txt likely\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt manager\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s\n",
            "0 tokenized_document_0.txt estimate\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt intended\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt taken\n",
            "0 tokenized_document_0.txt figuratively\n",
            "0 tokenized_document_0.txt rather\n",
            "0 tokenized_document_0.txt than\n",
            "0 tokenized_document_0.txt literally\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt serves\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt illustrate\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt fact\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt great\n",
            "0 tokenized_document_0.txt united\n",
            "0 tokenized_document_0.txt states\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt millions\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt young\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt middle-aged\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt old\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt content\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt plod\n",
            "0 tokenized_document_0.txt along\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt mediocre\n",
            "0 tokenized_document_0.txt sort\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt heedless\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt unmindful\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt fact\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt opportunity\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt possibilities\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt calling\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt calling\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt calling\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt come\n",
            "0 tokenized_document_0.txt up\n",
            "0 tokenized_document_0.txt higher\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt hundreds\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt thousands\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt other\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt engaged\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt sit\n",
            "0 tokenized_document_0.txt idly\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt while\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt trade\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt like\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt sands\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt hour\n",
            "0 tokenized_document_0.txt glass\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt slowly\n",
            "0 tokenized_document_0.txt ebbs\n",
            "0 tokenized_document_0.txt away\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt eventually\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt absorbed\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt progressive\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt neighbors\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt moneymaking\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt literature\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt still\n",
            "0 tokenized_document_0.txt another\n",
            "0 tokenized_document_0.txt vast\n",
            "0 tokenized_document_0.txt army\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt salesmen\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt clerks\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt wage-earners\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt classes\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt beginning\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt catch\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt glimpse\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt dawning\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt era\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt ever\n",
            "0 tokenized_document_0.txt known\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt an\n",
            "0 tokenized_document_0.txt era\n",
            "0 tokenized_document_0.txt impregnated\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt possibilities\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt opportunities\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt those\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt ready\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt wicks\n",
            "0 tokenized_document_0.txt trimmed\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt oil\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt lamps\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt earnest\n",
            "0 tokenized_document_0.txt latter\n",
            "0 tokenized_document_0.txt class\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt really\n",
            "0 tokenized_document_0.txt desirous\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt profiting\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt others\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt no\n",
            "0 tokenized_document_0.txt need\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt elaborating\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt possibilities\n",
            "0 tokenized_document_0.txt embodied\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt course\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt reading\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt administration\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt set\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt books\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt containing\n",
            "0 tokenized_document_0.txt valuable\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt data\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt many\n",
            "0 tokenized_document_0.txt subjects\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt thousands\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt pages\n",
            "0 tokenized_document_0.txt telling\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt story\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt illustrated\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt trained\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt whose\n",
            "0 tokenized_document_0.txt names\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt respected\n",
            "0 tokenized_document_0.txt everywhere\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt intended\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt reach\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt classes\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt absolutely\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt print\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt even\n",
            "0 tokenized_document_0.txt approach\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt begin\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt compare\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt value\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt reference\n",
            "0 tokenized_document_0.txt library\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt excel\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt complete\n",
            "0 tokenized_document_0.txt course\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt instruction\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt any\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt desirous\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt making\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt best\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt possibilities\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt opportunities\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt kaleidoscopic\n",
            "0 tokenized_document_0.txt age\n",
            "0 tokenized_document_0.txt through\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt now\n",
            "0 tokenized_document_0.txt moving\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt practical\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt best\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt practical\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt ideas\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt basis\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt good\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt long\n",
            "0 tokenized_document_0.txt since\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt generally\n",
            "0 tokenized_document_0.txt pooh-poohed\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt idea\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt employing\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt conduct\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt anything\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt taken\n",
            "0 tokenized_document_0.txt from\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt writings\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt experience\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt others\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt contained\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt remarkable\n",
            "0 tokenized_document_0.txt series\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt contributed\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt some\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt brightest\n",
            "0 tokenized_document_0.txt minds\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt today\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt however\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt these\n",
            "0 tokenized_document_0.txt days\n",
            "0 tokenized_document_0.txt unmistakably\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt hungering\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt thirsting\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt just\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt sort\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt literature\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt fills\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt long-felt\n",
            "0 tokenized_document_0.txt need\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt fills\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt exactly\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt completely\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt satisfactorily\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt being\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt author\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt work\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt salesmanship\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt had\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt countrywide\n",
            "0 tokenized_document_0.txt circulation\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt literally\n",
            "0 tokenized_document_0.txt besieged\n",
            "0 tokenized_document_0.txt by\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt everywhere\n",
            "0 tokenized_document_0.txt asking\n",
            "0 tokenized_document_0.txt me\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt recommend\n",
            "0 tokenized_document_0.txt books\n",
            "0 tokenized_document_0.txt treating\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt successful\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt methods\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt have\n",
            "0 tokenized_document_0.txt been\n",
            "0 tokenized_document_0.txt chagrined\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt find\n",
            "0 tokenized_document_0.txt how\n",
            "0 tokenized_document_0.txt limited\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt supply\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt formerly\n",
            "0 tokenized_document_0.txt was\n",
            "0 tokenized_document_0.txt prejudiced\n",
            "0 tokenized_document_0.txt against\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt sources\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt information\n",
            "0 tokenized_document_0.txt must\n",
            "0 tokenized_document_0.txt now\n",
            "0 tokenized_document_0.txt step\n",
            "0 tokenized_document_0.txt aside\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt way\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt unite\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt popular\n",
            "0 tokenized_document_0.txt demand\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt more\n",
            "0 tokenized_document_0.txt education\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt better\n",
            "0 tokenized_document_0.txt methods\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt afford\n",
            "0 tokenized_document_0.txt vs.\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt afford\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt me\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt says\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt no\n",
            "0 tokenized_document_0.txt patience\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt things\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt like\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt stroke\n",
            "0 tokenized_document_0.txt oar\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt lawn\n",
            "0 tokenized_document_0.txt mower\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt does\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt believe\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt moving\n",
            "0 tokenized_document_0.txt forward\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt progress\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt me\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt says\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt no\n",
            "0 tokenized_document_0.txt time\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt read\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt new\n",
            "0 tokenized_document_0.txt methods\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt principles\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt utterly\n",
            "0 tokenized_document_0.txt fails\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt perceive\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt familiarity\n",
            "0 tokenized_document_0.txt with\n",
            "0 tokenized_document_0.txt business\n",
            "0 tokenized_document_0.txt literature\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt kind\n",
            "0 tokenized_document_0.txt means\n",
            "0 tokenized_document_0.txt pecuniary\n",
            "0 tokenized_document_0.txt advancement\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt me\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt says\n",
            "0 tokenized_document_0.txt he\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt afford\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt invest\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt set\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt books\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt will\n",
            "0 tokenized_document_0.txt show\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt apparently\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt afford\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt waste\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt energy\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt misdirected\n",
            "0 tokenized_document_0.txt effort\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt energy\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt effort\n",
            "0 tokenized_document_0.txt which\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt every\n",
            "0 tokenized_document_0.txt wage-earner\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt tradesman\n",
            "0 tokenized_document_0.txt both\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt stock\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt trade\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt invested\n",
            "0 tokenized_document_0.txt capital\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt [\n",
            "0 tokenized_document_0.txt sidenote\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt failures\n",
            "0 tokenized_document_0.txt unnecessary\n",
            "0 tokenized_document_0.txt ]\n",
            "0 tokenized_document_0.txt someone\n",
            "0 tokenized_document_0.txt has\n",
            "0 tokenized_document_0.txt said\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt three\n",
            "0 tokenized_document_0.txt kinds\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt people\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt world\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt can\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt ts\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt won\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt ts\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt wills\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt first\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt everything\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt second\n",
            "0 tokenized_document_0.txt oppose\n",
            "0 tokenized_document_0.txt everything\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt third\n",
            "0 tokenized_document_0.txt succeed\n",
            "0 tokenized_document_0.txt at\n",
            "0 tokenized_document_0.txt everything.\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt would\n",
            "0 tokenized_document_0.txt add\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt fourth\n",
            "0 tokenized_document_0.txt kind\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt largest\n",
            "0 tokenized_document_0.txt class\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt all\n",
            "0 tokenized_document_0.txt --\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt don\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt t\n",
            "0 tokenized_document_0.txt trys\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt oh-what\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s-the-use\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt it-doesn\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt t-interest-me\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt sort\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt people\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt name\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt legion\n",
            "0 tokenized_document_0.txt ;\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt fault\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt confidence\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt knowledge\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt greatest\n",
            "0 tokenized_document_0.txt inspiration\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt confidence\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt found\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt earth\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt may\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt personally\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt held\n",
            "0 tokenized_document_0.txt in\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt hope-paralyzing\n",
            "0 tokenized_document_0.txt bondage\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt produces\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt oh-what\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt s-the-use\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt “\n",
            "0 tokenized_document_0.txt i\n",
            "0 tokenized_document_0.txt ’\n",
            "0 tokenized_document_0.txt m-not-interested\n",
            "0 tokenized_document_0.txt ”\n",
            "0 tokenized_document_0.txt germ\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt but\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt not\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt you\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt exceptional\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt people\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt reason\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt such\n",
            "0 tokenized_document_0.txt persons\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt just\n",
            "0 tokenized_document_0.txt about\n",
            "0 tokenized_document_0.txt what\n",
            "0 tokenized_document_0.txt luck\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt good\n",
            "0 tokenized_document_0.txt fortune\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt chance\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt succeeding\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt fortune\n",
            "0 tokenized_document_0.txt favors\n",
            "0 tokenized_document_0.txt them\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt failing\n",
            "0 tokenized_document_0.txt if\n",
            "0 tokenized_document_0.txt they\n",
            "0 tokenized_document_0.txt are\n",
            "0 tokenized_document_0.txt left\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt depend\n",
            "0 tokenized_document_0.txt upon\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt own\n",
            "0 tokenized_document_0.txt resources\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt result\n",
            "0 tokenized_document_0.txt :\n",
            "0 tokenized_document_0.txt nine\n",
            "0 tokenized_document_0.txt fail\n",
            "0 tokenized_document_0.txt where\n",
            "0 tokenized_document_0.txt one\n",
            "0 tokenized_document_0.txt succeeds\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt very\n",
            "0 tokenized_document_0.txt fortunate\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt indeed\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt for\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt men\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt so\n",
            "0 tokenized_document_0.txt much\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt their\n",
            "0 tokenized_document_0.txt happiness\n",
            "0 tokenized_document_0.txt depends\n",
            "0 tokenized_document_0.txt upon\n",
            "0 tokenized_document_0.txt success\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt there\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt nothing\n",
            "0 tokenized_document_0.txt on\n",
            "0 tokenized_document_0.txt earth\n",
            "0 tokenized_document_0.txt quite\n",
            "0 tokenized_document_0.txt so\n",
            "0 tokenized_document_0.txt terrible\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt think\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt as\n",
            "0 tokenized_document_0.txt failure\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt especially\n",
            "0 tokenized_document_0.txt that\n",
            "0 tokenized_document_0.txt due\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt lack\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt effort\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt unless\n",
            "0 tokenized_document_0.txt possibly\n",
            "0 tokenized_document_0.txt it\n",
            "0 tokenized_document_0.txt be\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt failure\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt a\n",
            "0 tokenized_document_0.txt man\n",
            "0 tokenized_document_0.txt who\n",
            "0 tokenized_document_0.txt lacks\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt courage\n",
            "0 tokenized_document_0.txt or\n",
            "0 tokenized_document_0.txt initiative\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt try\n",
            "0 tokenized_document_0.txt to\n",
            "0 tokenized_document_0.txt make\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt of\n",
            "0 tokenized_document_0.txt himself\n",
            "0 tokenized_document_0.txt ,\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt thus\n",
            "0 tokenized_document_0.txt lets\n",
            "0 tokenized_document_0.txt his\n",
            "0 tokenized_document_0.txt best\n",
            "0 tokenized_document_0.txt opportunities\n",
            "0 tokenized_document_0.txt escape\n",
            "0 tokenized_document_0.txt him\n",
            "0 tokenized_document_0.txt .\n",
            "0 tokenized_document_0.txt and\n",
            "0 tokenized_document_0.txt this\n",
            "0 tokenized_document_0.txt last\n",
            "0 tokenized_document_0.txt is\n",
            "0 tokenized_document_0.txt really\n",
            "0 tokenized_document_0.txt the\n",
            "0 tokenized_document_0.txt most\n",
            "0 tokenized_document_0.txt "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-0cf6737d3316>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Tokenize by splitting on whitespace and standardize to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_content\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minverted_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentID\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use set to avoid duplicate document IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    616\u001b[0m                 )\n\u001b[1;32m    617\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     def send_multipart(\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming all_books is a list of Path objects\n",
        "document_mapping = {documentID: document.name for documentID, document in enumerate(all_books)}"
      ],
      "metadata": {
        "id": "FGgDBLdkL5qN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup_word(word):\n",
        "    if word in inverted_index:\n",
        "        document_ids = inverted_index[word]\n",
        "        print(f\"Congratulations! I could find the word '{word}' in the following document ID(s): {document_ids}\")\n",
        "        for doc_id in document_ids:\n",
        "            print(f\"Document ID: {doc_id}, Document Name: {document_mapping[doc_id]}\")\n",
        "    else:\n",
        "        print(f\"I'm sorry, I couldn't find the word '{word}' in any document.\")"
      ],
      "metadata": {
        "id": "NDtUp88wEiaL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lookup_word(\"should\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edXAlaKUKmy0",
        "outputId": "625102b5-0ca9-4ec5-f53c-84960460a4c8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congratulations! I could find the word 'should' in the following document ID(s): {0}\n",
            "Document ID: 0, Document Name: Business Administration- Theory, Practice and Application.txt\n"
          ]
        }
      ]
    }
  ]
}